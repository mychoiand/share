{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:45:36.795852Z",
     "iopub.status.busy": "2026-02-26T13:45:36.794035Z",
     "iopub.status.idle": "2026-02-26T13:45:36.807109Z",
     "shell.execute_reply": "2026-02-26T13:45:36.804668Z",
     "shell.execute_reply.started": "2026-02-26T13:45:36.795777Z"
    },
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Intro to Generating and Executing Python Code with Gemini 2.5 Flash\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fcode-execution%2Fintro_code_execution.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/code-execution/intro_code_execution.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "| Author(s) |  [Kristopher Overholt](https://github.com/koverholt/) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook introduces the code execution capabilities of the [Gemini 2.5 Flash model](https://cloud.google.com/vertex-ai/generative-ai/docs/models), a new multimodal generative AI model from Google [DeepMind](https://deepmind.google/). Gemini 2.5 Flash offers improvements in speed, quality, and advanced reasoning capabilities including enhanced understanding, coding, and instruction following.\n",
    "\n",
    "## Code Execution\n",
    "\n",
    "A key feature of this model is [code execution](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution), which is the ability to generate and execute Python code directly within the API. If you want the API to generate and run Python code and return the results, you can use code execution as demonstrated in this notebook.\n",
    "\n",
    "This code execution capability enables the model to generate code, execute and observe the results, correct the code if needed, and learn iteratively from the results until it produces a final output. This is particularly useful for applications that involve code-based reasoning such as solving mathematical equations or processing text.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this tutorial, you will learn how to generate and execute code using the Gemini API in Vertex AI and the Google Gen AI SDK for Python with the Gemini 2.5 Flash model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Generating and running sample Python code from text prompts\n",
    "- Exploring data using code execution in multi-turn chats\n",
    "- Using code execution in streaming sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:46:18.644255Z",
     "iopub.status.busy": "2026-02-26T13:46:18.642913Z",
     "iopub.status.idle": "2026-02-26T13:46:24.075502Z",
     "shell.execute_reply": "2026-02-26T13:46:24.073880Z",
     "shell.execute_reply.started": "2026-02-26T13:46:18.644196Z"
    },
    "id": "tFy3H3aPgx12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fggiCx13zxX"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:46:49.601712Z",
     "iopub.status.busy": "2026-02-26T13:46:49.599509Z",
     "iopub.status.idle": "2026-02-26T13:46:51.949422Z",
     "shell.execute_reply": "2026-02-26T13:46:51.948210Z",
     "shell.execute_reply.started": "2026-02-26T13:46:49.601649Z"
    },
    "id": "JbrnA9yv3zMC"
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Markdown, display\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig, Tool, ToolCodeExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXiC1rOE3gSZ"
   },
   "source": [
    "### Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- [Google AI for Developers](https://ai.google.dev/gemini-api/docs): Experiment, prototype, and deploy small projects.\n",
    "- [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs): Build enterprise-ready projects on Google Cloud.\n",
    "The Google Gen AI SDK provides a unified interface to these two API services.\n",
    "\n",
    "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and create client\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:49:32.391799Z",
     "iopub.status.busy": "2026-02-26T13:49:32.387203Z",
     "iopub.status.idle": "2026-02-26T13:49:32.606067Z",
     "shell.execute_reply": "2026-02-26T13:49:32.604614Z",
     "shell.execute_reply.started": "2026-02-26T13:49:32.391732Z"
    },
    "id": "3Ab5NQwr4B8j"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-01-26236439ed1e\"\n",
    "LOCATION = \"global\"\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1vpnyk-q-fz"
   },
   "source": [
    "## Working with code execution in Gemini 2.5 Flash\n",
    "\n",
    "### Load the Gemini model\n",
    "\n",
    "The following code loads the Gemini 2.5 Flash model. You can learn about all Gemini models on Vertex AI by visiting the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:49:34.069044Z",
     "iopub.status.busy": "2026-02-26T13:49:34.068509Z",
     "iopub.status.idle": "2026-02-26T13:49:34.075617Z",
     "shell.execute_reply": "2026-02-26T13:49:34.073631Z",
     "shell.execute_reply.started": "2026-02-26T13:49:34.069004Z"
    },
    "id": "L8gLWcOFqqF2"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-jdBwXlM67j"
   },
   "source": [
    "### Define the code execution tool\n",
    "\n",
    "The following code initializes the code execution tool by passing `code_execution` in a `Tool` definition.\n",
    "\n",
    "Later we'll register this tool with the model that it can use to generate and run Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:49:37.301258Z",
     "iopub.status.busy": "2026-02-26T13:49:37.300729Z",
     "iopub.status.idle": "2026-02-26T13:49:37.307734Z",
     "shell.execute_reply": "2026-02-26T13:49:37.306190Z",
     "shell.execute_reply.started": "2026-02-26T13:49:37.301218Z"
    },
    "id": "BFxIcGkxbq3_"
   },
   "outputs": [],
   "source": [
    "code_execution_tool = Tool(code_execution=ToolCodeExecution())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZgn5tm-NCfH"
   },
   "source": [
    "### Generate and execute code\n",
    "\n",
    "The following code sends a prompt to the Gemini model, asking it to generate and execute Python code to calculate the sum of the first 50 prime numbers. The code execution tool is passed in so the model can generate and run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:49:39.109601Z",
     "iopub.status.busy": "2026-02-26T13:49:39.109061Z",
     "iopub.status.idle": "2026-02-26T13:49:42.119914Z",
     "shell.execute_reply": "2026-02-26T13:49:42.118409Z",
     "shell.execute_reply.started": "2026-02-26T13:49:39.109563Z"
    },
    "id": "b52qMx0IGA0K"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"What is the sum of the first 50 prime numbers?\n",
    "Generate and run code for the calculation.\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=PROMPT,\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-mfiMNasgqH"
   },
   "source": [
    "### View the generated code\n",
    "\n",
    "The following code iterates through the response and displays any generated Python code by checking for `part.executable_code` in the response parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:49:42.127166Z",
     "iopub.status.busy": "2026-02-26T13:49:42.126333Z",
     "iopub.status.idle": "2026-02-26T13:49:42.140877Z",
     "shell.execute_reply": "2026-02-26T13:49:42.139404Z",
     "shell.execute_reply.started": "2026-02-26T13:49:42.127127Z"
    },
    "id": "J5mcXw6ZraLS"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```py\n",
       "import math\n",
       "\n",
       "def is_prime(n):\n",
       "    if n < 2:\n",
       "        return False\n",
       "    for i in range(2, int(math.sqrt(n)) + 1):\n",
       "        if n % i == 0:\n",
       "            return False\n",
       "    return True\n",
       "\n",
       "prime_numbers = []\n",
       "num = 2\n",
       "while len(prime_numbers) < 50:\n",
       "    if is_prime(num):\n",
       "        prime_numbers.append(num)\n",
       "    num += 1\n",
       "\n",
       "sum_of_primes = sum(prime_numbers)\n",
       "print(f'{prime_numbers=}')\n",
       "print(f'{sum_of_primes=}')\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.executable_code:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"\n",
    "```py\n",
    "{part.executable_code.code}\n",
    "```\n",
    "\"\"\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppumif-94xTF"
   },
   "source": [
    "### View the code execution results\n",
    "\n",
    "The following code iterates through the response and displays the execution result and outcome by checking for `part.code_execution_result` in the response parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T13:55:30.665735Z",
     "iopub.status.busy": "2026-02-26T13:55:30.664633Z",
     "iopub.status.idle": "2026-02-26T13:55:30.675405Z",
     "shell.execute_reply": "2026-02-26T13:55:30.674205Z",
     "shell.execute_reply.started": "2026-02-26T13:55:30.665671Z"
    },
    "id": "J891OBjc4xn9"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "`prime_numbers=[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\n",
       "sum_of_primes=5117\n",
       "`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome: Outcome.OUTCOME_OK\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.code_execution_result:\n",
    "        display(Markdown(f\"`{part.code_execution_result.output}`\"))\n",
    "        print(\"\\nOutcome:\", part.code_execution_result.outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u_XuZlMnH9S"
   },
   "source": [
    "Great! Now you have the answer (`5117`) as well as the generated (and verified via execution!) Python code.\n",
    "\n",
    "At this point in your application, you would save the output code, result, or outcome and display it to the end-user or use it downstream in your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uJ-Fk1I_AH8"
   },
   "source": [
    "### Code execution in a chat session\n",
    "\n",
    "This section shows how to use code execution in an interactive chat with history using the Gemini API.\n",
    "\n",
    "You can use `client.chats.create` to create a chat session and passes in the code execution tool, enabling the model to generate and run code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:04:42.888128Z",
     "iopub.status.busy": "2026-02-26T14:04:42.887517Z",
     "iopub.status.idle": "2026-02-26T14:04:42.898110Z",
     "shell.execute_reply": "2026-02-26T14:04:42.896415Z",
     "shell.execute_reply.started": "2026-02-26T14:04:42.888072Z"
    },
    "id": "puL91bq7tirC"
   },
   "outputs": [],
   "source": [
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "        max_output_tokens=10240,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bmu4bSApoECT"
   },
   "source": [
    "You'll start the chat by asking the model to generate sample time series data with noise and then output a sample of 10 data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:04:44.890859Z",
     "iopub.status.busy": "2026-02-26T14:04:44.890292Z",
     "iopub.status.idle": "2026-02-26T14:04:53.260076Z",
     "shell.execute_reply": "2026-02-26T14:04:53.258930Z",
     "shell.execute_reply.started": "2026-02-26T14:04:44.890821Z"
    },
    "id": "8iyq5sKCtstH"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Create sample time series data of temperature vs. time in a test furnace.\n",
    "Add noise to the data. Output a sample of 10 data points from the time series data.\"\"\"\n",
    "\n",
    "response = chat.send_message(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVhCKKBioJga"
   },
   "source": [
    "Now you can iterate through the response to display any generated Python code and execution results by checking for `part.executable_code` and `part.code_execution_result` in the response parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:04:53.263372Z",
     "iopub.status.busy": "2026-02-26T14:04:53.262346Z",
     "iopub.status.idle": "2026-02-26T14:04:53.276165Z",
     "shell.execute_reply": "2026-02-26T14:04:53.275059Z",
     "shell.execute_reply.started": "2026-02-26T14:04:53.263312Z"
    },
    "id": "8pjwEGzft29N"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```py\n",
       "import numpy as np\n",
       "\n",
       "# --- Parameters for the furnace temperature profile ---\n",
       "T_start = 25.0  # Initial temperature (e.g., room temperature in Celsius)\n",
       "T_setpoint = 500.0 # Target furnace temperature\n",
       "heating_rate = 25.0 # Degrees Celsius per unit of time (e.g., per minute)\n",
       "noise_amplitude = 3.0 # Standard deviation of the noise in Celsius\n",
       "\n",
       "# --- Time parameters ---\n",
       "total_duration = 30.0 # Total duration of the simulation (e.g., in minutes)\n",
       "sampling_interval = 0.5 # Time step between measurements (e.g., 30 seconds)\n",
       "\n",
       "# Generate time points\n",
       "time = np.arange(0, total_duration + sampling_interval, sampling_interval)\n",
       "\n",
       "# Calculate base temperature profile\n",
       "# The temperature rises linearly until it reaches the setpoint, then stays constant.\n",
       "base_temperature = np.minimum(T_setpoint, T_start + heating_rate * time)\n",
       "\n",
       "# Add Gaussian noise to the base temperature\n",
       "noise = np.random.normal(0, noise_amplitude, len(time))\n",
       "noisy_temperature = base_temperature + noise\n",
       "\n",
       "# --- Select 10 sample data points ---\n",
       "# To get a representative sample, we'll pick 10 evenly spaced points.\n",
       "# If there are fewer than 10 points, we'll just take all of them.\n",
       "num_total_points = len(time)\n",
       "if num_total_points >= 10:\n",
       "    indices = np.linspace(0, num_total_points - 1, 10, dtype=int)\n",
       "else:\n",
       "    indices = np.arange(num_total_points)\n",
       "\n",
       "sample_time = time[indices]\n",
       "sample_temperature = noisy_temperature[indices]\n",
       "\n",
       "print(\"Sample Time Series Data (Time vs. Temperature with Noise):\")\n",
       "print(\"-------------------------------------------------------\")\n",
       "for i in range(len(sample_time)):\n",
       "    print(f\"Time: {sample_time[i]:.1f} min, Temperature: {sample_temperature[i]:.2f} °C\")\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`Sample Time Series Data (Time vs. Temperature with Noise):\n",
       "-------------------------------------------------------\n",
       "Time: 0.0 min, Temperature: 22.27 °C\n",
       "Time: 3.0 min, Temperature: 94.90 °C\n",
       "Time: 6.5 min, Temperature: 186.45 °C\n",
       "Time: 10.0 min, Temperature: 272.90 °C\n",
       "Time: 13.0 min, Temperature: 351.89 °C\n",
       "Time: 16.5 min, Temperature: 440.24 °C\n",
       "Time: 20.0 min, Temperature: 499.32 °C\n",
       "Time: 23.0 min, Temperature: 498.27 °C\n",
       "Time: 26.5 min, Temperature: 502.08 °C\n",
       "Time: 30.0 min, Temperature: 503.36 °C\n",
       "`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome: Outcome.OUTCOME_OK\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.executable_code:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"\n",
    "```py\n",
    "{part.executable_code.code}\n",
    "```\n",
    "\"\"\"\n",
    "            )\n",
    "        )\n",
    "    if part.code_execution_result:\n",
    "        display(Markdown(f\"`{part.code_execution_result.output}`\"))\n",
    "        print(\"\\nOutcome:\", part.code_execution_result.outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AHoGmDBQuxn"
   },
   "source": [
    "Now you can ask the model to add a smoothed data series to the time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:04:53.358938Z",
     "iopub.status.busy": "2026-02-26T14:04:53.358432Z",
     "iopub.status.idle": "2026-02-26T14:05:00.835706Z",
     "shell.execute_reply": "2026-02-26T14:05:00.834604Z",
     "shell.execute_reply.started": "2026-02-26T14:04:53.358906Z"
    },
    "id": "alR_tq3pss7j"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"Now add a data series that smooths the sample data.\"\n",
    "\n",
    "response = chat.send_message(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnSlnA5FQ9UH"
   },
   "source": [
    "And then display the generated Python code and execution results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:05:00.838660Z",
     "iopub.status.busy": "2026-02-26T14:05:00.837967Z",
     "iopub.status.idle": "2026-02-26T14:05:00.849841Z",
     "shell.execute_reply": "2026-02-26T14:05:00.848722Z",
     "shell.execute_reply.started": "2026-02-26T14:05:00.838625Z"
    },
    "id": "uMXRpE0NtRYC"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```py\n",
       "import numpy as np\n",
       "\n",
       "# --- Parameters for the furnace temperature profile (same as before) ---\n",
       "T_start = 25.0\n",
       "T_setpoint = 500.0\n",
       "heating_rate = 25.0\n",
       "noise_amplitude = 3.0\n",
       "\n",
       "# --- Time parameters (same as before) ---\n",
       "total_duration = 30.0\n",
       "sampling_interval = 0.5\n",
       "\n",
       "# Generate time points\n",
       "time = np.arange(0, total_duration + sampling_interval, sampling_interval)\n",
       "\n",
       "# Calculate base temperature profile\n",
       "base_temperature = np.minimum(T_setpoint, T_start + heating_rate * time)\n",
       "\n",
       "# Add Gaussian noise to the base temperature\n",
       "noise = np.random.normal(0, noise_amplitude, len(time))\n",
       "noisy_temperature = base_temperature + noise\n",
       "\n",
       "# --- Select 10 sample data points (same as before) ---\n",
       "num_total_points = len(time)\n",
       "if num_total_points >= 10:\n",
       "    indices = np.linspace(0, num_total_points - 1, 10, dtype=int)\n",
       "else:\n",
       "    indices = np.arange(num_total_points)\n",
       "\n",
       "sample_time = time[indices]\n",
       "sample_temperature = noisy_temperature[indices]\n",
       "\n",
       "# --- Add Smoothing ---\n",
       "# Define the window size for the moving average\n",
       "window_size = 3\n",
       "\n",
       "# Apply a moving average filter\n",
       "# np.convolve is used for convolution. We create a window of ones and normalize it.\n",
       "# 'valid' mode would shorten the array, 'same' mode pads and keeps the same length.\n",
       "# For 'same' mode, padding is applied to the ends.\n",
       "# For a simple moving average, we can manually handle edges or use a function like pandas.rolling.mean\n",
       "# or scipy.ndimage.uniform_filter1d.\n",
       "# For this small sample, a manual approach or a simple convolution with 'same' mode is fine.\n",
       "\n",
       "# Manual moving average for 'same' mode\n",
       "smoothed_temperature = np.zeros_like(sample_temperature)\n",
       "for i in range(len(sample_temperature)):\n",
       "    start_idx = max(0, i - window_size // 2)\n",
       "    end_idx = min(len(sample_temperature), i + window_size // 2 + 1)\n",
       "    smoothed_temperature[i] = np.mean(sample_temperature[start_idx:end_idx])\n",
       "\n",
       "\n",
       "print(\"Sample Time Series Data (Time vs. Temperature with Noise and Smoothed):\")\n",
       "print(\"--------------------------------------------------------------------\")\n",
       "print(f\"{'Time (min)':<12} | {'Noisy Temp (°C)':<18} | {'Smoothed Temp (°C)':<18}\")\n",
       "print(\"--------------------------------------------------------------------\")\n",
       "for i in range(len(sample_time)):\n",
       "    print(f\"{sample_time[i]:<12.1f} | {sample_temperature[i]:<18.2f} | {smoothed_temperature[i]:<18.2f}\")\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`Sample Time Series Data (Time vs. Temperature with Noise and Smoothed):\n",
       "--------------------------------------------------------------------\n",
       "Time (min)   | Noisy Temp (°C)    | Smoothed Temp (°C)\n",
       "--------------------------------------------------------------------\n",
       "0.0          | 23.04              | 64.08             \n",
       "3.0          | 105.13             | 105.91            \n",
       "6.5          | 189.58             | 188.00            \n",
       "10.0         | 269.31             | 268.38            \n",
       "13.0         | 346.25             | 352.02            \n",
       "16.5         | 440.51             | 428.90            \n",
       "20.0         | 499.95             | 482.41            \n",
       "23.0         | 506.77             | 501.99            \n",
       "26.5         | 499.27             | 501.06            \n",
       "30.0         | 497.14             | 498.20            \n",
       "`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome: Outcome.OUTCOME_OK\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.executable_code:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"\n",
    "```py\n",
    "{part.executable_code.code}\n",
    "```\n",
    "\"\"\"\n",
    "            )\n",
    "        )\n",
    "    if part.code_execution_result:\n",
    "        display(Markdown(f\"`{part.code_execution_result.output}`\"))\n",
    "        print(\"\\nOutcome:\", part.code_execution_result.outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4VacTEyQ4lD"
   },
   "source": [
    "Finally, you can ask the model to generate descriptive statistics for the time series data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:05:03.285830Z",
     "iopub.status.busy": "2026-02-26T14:05:03.285311Z",
     "iopub.status.idle": "2026-02-26T14:05:10.552187Z",
     "shell.execute_reply": "2026-02-26T14:05:10.550885Z",
     "shell.execute_reply.started": "2026-02-26T14:05:03.285770Z"
    },
    "id": "dmhPzmP8tywL"
   },
   "outputs": [],
   "source": [
    "PROMPT = \"Now generate and output descriptive statistics on the time series data.\"\n",
    "\n",
    "response = chat.send_message(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1t_zA5jRHsB"
   },
   "source": [
    "And then display the generated Python code and execution results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:05:47.173114Z",
     "iopub.status.busy": "2026-02-26T14:05:47.172358Z",
     "iopub.status.idle": "2026-02-26T14:05:47.187676Z",
     "shell.execute_reply": "2026-02-26T14:05:47.186593Z",
     "shell.execute_reply.started": "2026-02-26T14:05:47.172931Z"
    },
    "id": "hIsMH3fPuKr5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```py\n",
       "import numpy as np\n",
       "\n",
       "# --- Parameters for the furnace temperature profile (same as before) ---\n",
       "T_start = 25.0\n",
       "T_setpoint = 500.0\n",
       "heating_rate = 25.0\n",
       "noise_amplitude = 3.0\n",
       "\n",
       "# --- Time parameters (same as before) ---\n",
       "total_duration = 30.0\n",
       "sampling_interval = 0.5\n",
       "\n",
       "# Generate time points\n",
       "time = np.arange(0, total_duration + sampling_interval, sampling_interval)\n",
       "\n",
       "# Calculate base temperature profile\n",
       "base_temperature = np.minimum(T_setpoint, T_start + heating_rate * time)\n",
       "\n",
       "# Add Gaussian noise to the base temperature\n",
       "noise = np.random.normal(0, noise_amplitude, len(time))\n",
       "noisy_temperature = base_temperature + noise\n",
       "\n",
       "# --- Select 10 sample data points (same as before) ---\n",
       "num_total_points = len(time)\n",
       "if num_total_points >= 10:\n",
       "    indices = np.linspace(0, num_total_points - 1, 10, dtype=int)\n",
       "else:\n",
       "    indices = np.arange(num_total_points)\n",
       "\n",
       "sample_time = time[indices]\n",
       "sample_temperature = noisy_temperature[indices]\n",
       "\n",
       "# --- Add Smoothing (same as before) ---\n",
       "window_size = 3\n",
       "smoothed_temperature = np.zeros_like(sample_temperature)\n",
       "for i in range(len(sample_temperature)):\n",
       "    start_idx = max(0, i - window_size // 2)\n",
       "    end_idx = min(len(sample_temperature), i + window_size // 2 + 1)\n",
       "    smoothed_temperature[i] = np.mean(sample_temperature[start_idx:end_idx])\n",
       "\n",
       "# --- Generate Descriptive Statistics ---\n",
       "def get_descriptive_stats(data, name):\n",
       "    stats = {\n",
       "        \"Series\": name,\n",
       "        \"Count\": len(data),\n",
       "        \"Mean\": np.mean(data),\n",
       "        \"Median\": np.median(data),\n",
       "        \"Std Dev\": np.std(data),\n",
       "        \"Min\": np.min(data),\n",
       "        \"Max\": np.max(data),\n",
       "        \"Range\": np.max(data) - np.min(data)\n",
       "    }\n",
       "    return stats\n",
       "\n",
       "stats_noisy = get_descriptive_stats(sample_temperature, \"Noisy Temperature\")\n",
       "stats_smoothed = get_descriptive_stats(smoothed_temperature, \"Smoothed Temperature\")\n",
       "\n",
       "print(\"Descriptive Statistics for Temperature Data:\")\n",
       "print(\"--------------------------------------------\")\n",
       "\n",
       "# Print header\n",
       "print(f\"{'Statistic':<15} | {stats_noisy['Series']:<20} | {stats_smoothed['Series']:<20}\")\n",
       "print(\"-\" * 60)\n",
       "\n",
       "# Print values\n",
       "for key in [\"Count\", \"Mean\", \"Median\", \"Std Dev\", \"Min\", \"Max\", \"Range\"]:\n",
       "    print(f\"{key:<15} | {stats_noisy[key]:<20.2f} | {stats_smoothed[key]:<20.2f}\")\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`Descriptive Statistics for Temperature Data:\n",
       "--------------------------------------------\n",
       "Statistic       | Noisy Temperature    | Smoothed Temperature\n",
       "------------------------------------------------------------\n",
       "Count           | 10.00                | 10.00               \n",
       "Mean            | 338.68               | 339.86              \n",
       "Median          | 397.58               | 393.84              \n",
       "Std Dev         | 172.37               | 162.93              \n",
       "Min             | 24.63                | 61.89               \n",
       "Max             | 502.91               | 501.07              \n",
       "Range           | 478.28               | 439.18              \n",
       "`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outcome: Outcome.OUTCOME_OK\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "    if part.executable_code:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"\n",
    "```py\n",
    "{part.executable_code.code}\n",
    "```\n",
    "\"\"\"\n",
    "            )\n",
    "        )\n",
    "    if part.code_execution_result:\n",
    "        display(Markdown(f\"`{part.code_execution_result.output}`\"))\n",
    "        print(\"\\nOutcome:\", part.code_execution_result.outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBbNyWtDRZto"
   },
   "source": [
    "This chat example demonstrates how you can use the Gemini API with code execution as a powerful tool for exploratory data analysis and more. Go forth and adapt this approach to your own projects and use cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl6KG5Ufu5XQ"
   },
   "source": [
    "### Code execution in a streaming session\n",
    "\n",
    "You can also use the code execution functionality with streaming output from the Gemini API.\n",
    "\n",
    "The following code demonstrates how the Gemini API can generate and execute code while streaming the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-26T14:06:01.319676Z",
     "iopub.status.busy": "2026-02-26T14:06:01.318992Z",
     "iopub.status.idle": "2026-02-26T14:06:08.187169Z",
     "shell.execute_reply": "2026-02-26T14:06:08.185996Z",
     "shell.execute_reply.started": "2026-02-26T14:06:01.319615Z"
    },
    "id": "gTNMMLkNu5JH"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Code stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    ```py\n",
       "    import random\n",
       "\n",
       "# Step 1: Create a list of potential names\n",
       "potential_names = [\n",
       "    \"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Heidi\", \"Ivan\", \"Judy\",\n",
       "    \"Kevin\", \"Linda\", \"Mike\", \"Nancy\", \"Oscar\", \"Pamela\", \"Quinn\", \"Rachel\", \"Steve\", \"Tina\",\n",
       "    \"Uma\", \"Victor\", \"Wendy\", \"Xavier\", \"Yara\", \"Zoe\", \"Adam\", \"Anna\", \"Brian\", \"Carla\",\n",
       "    \"Daniel\", \"Diana\", \"Ethan\", \"Hannah\", \"Isabella\", \"Jacob\", \"Karen\", \"Laura\", \"Mark\",\n",
       "    \"Maria\", \"Nathan\", \"Olivia\", \"Patrick\", \"Paula\", \"Robert\", \"Sarah\", \"Thomas\", \"Victoria\",\n",
       "    \"William\", \"Samantha\", \"Alexander\", \"Amanda\", \"Andrew\", \"Angela\", \"Anthony\", \"Barbara\",\n",
       "    \"Benjamin\", \"Brenda\", \"Catherine\", \"Charles\", \"Christine\", \"Christopher\", \"Cynthia\",\n",
       "    \"Deborah\", \"Dennis\", \"Donald\", \"Donna\", \"Dorothy\", \"Edward\", \"Elizabeth\", \"Emily\",\n",
       "    \"Eric\", \"Evelyn\", \"Florence\", \"Frances\", \"Gary\", \"George\", \"Georgia\", \"Gerald\",\n",
       "    \"Gloria\", \"Gregory\", \"Harold\", \"Helen\", \"Henry\", \"Jacqueline\", \"James\", \"Janet\",\n",
       "    \"Jason\", \"Jeffrey\", \"Jennifer\", \"Jerry\", \"Jessica\", \"Joan\", \"Joe\", \"John\", \"Jonathan\",\n",
       "    \"Jose\", \"Joseph\", \"Joshua\", \"Joyce\", \"Juan\", \"Judith\", \"Julia\", \"Julie\", \"Justin\",\n",
       "    \"Kathleen\", \"Kelly\", \"Kenneth\", \"Kimberly\", \"Larry\", \"Lawrence\", \"Lisa\", \"Louis\",\n",
       "    \"Louise\", \"Margaret\", \"Marilyn\", \"Martha\", \"Mary\", \"Matthew\", \"Megan\", \"Melissa\",\n",
       "    \"Michael\", \"Michelle\", \"Nancy\", \"Nicholas\", \"Nicole\", \"Patricia\", \"Paul\", \"Peter\",\n",
       "    \"Philip\", \"Raymond\", \"Rebecca\", \"Richard\", \"Ronald\", \"Rose\", \"Roy\", \"Russell\",\n",
       "    \"Sandra\", \"Scott\", \"Sharon\", \"Shirley\", \"Stephen\", \"Stephanie\", \"Steven\", \"Susan\",\n",
       "    \"Teresa\", \"Terry\", \"Theresa\", \"Timothy\", \"Virginia\", \"Walter\", \"Wayne\", \"William\"\n",
       "]\n",
       "\n",
       "# Step 2: Generate 20 random names\n",
       "random_names = random.sample(potential_names, 20)\n",
       "print(f\"Original list of 20 random names:\\n{random_names}\\n\")\n",
       "\n",
       "# Step 3 & 4: Initialize an empty list for names containing 'a' and iterate\n",
       "names_with_a = []\n",
       "for name in random_names:\n",
       "    # Step 5: Check if the name contains 'a' (case-insensitive)\n",
       "    if 'a' in name.lower():\n",
       "        # Step 6: If it does, add it to the new list\n",
       "        names_with_a.append(name)\n",
       "\n",
       "# Step 7: Output the number of names that contain 'a'\n",
       "print(f\"Number of names containing 'a': {len(names_with_a)}\\n\")\n",
       "\n",
       "# Step 8: Show the new list\n",
       "print(f\"List of names containing 'a':\\n{names_with_a}\")\n",
       "    ```\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Code result"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "    ```\n",
       "    Original list of 20 random names:\n",
       "['Eric', 'Larry', 'Rose', 'David', 'Jason', 'Hannah', 'Rebecca', 'Scott', 'Gloria', 'Emily', 'Marilyn', 'Nathan', 'Dennis', 'Daniel', 'Henry', 'Theresa', 'Donald', 'William', 'Russell', 'Eve']\n",
       "\n",
       "Number of names containing 'a': 12\n",
       "\n",
       "List of names containing 'a':\n",
       "['Larry', 'David', 'Jason', 'Hannah', 'Rebecca', 'Gloria', 'Marilyn', 'Nathan', 'Daniel', 'Theresa', 'Donald', 'William']\n",
       "\n",
       "    ```\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Natural language stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Natural language stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " is the list of 20 random names, followed by a new list containing only the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Natural language stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " names with the letter 'a', the count of those names, and finally the list itself.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Natural language stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Original list of 20 random names:\n",
       "['Eric', 'Larry', 'Rose', 'David', 'Jason', 'Hannah', 'Rebecca', 'Scott', 'Gloria',"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Natural language stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " 'Emily', 'Marilyn', 'Nathan', 'Dennis', 'Daniel', 'Henry', 'Theresa', 'Donald', 'William', 'Russell', 'Eve']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Natural language stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Number of names containing 'a': 12\n",
       "\n",
       "List of names containing 'a':\n",
       "['Larry', 'David', 'Jason', 'Hannah', 'Rebecca', 'Gloria', 'Marilyn', 'Nathan', 'Daniel"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Natural language stream"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "', 'Theresa', 'Donald', 'William']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROMPT = \"\"\"Generate a list of 20 random names, then create a new list with just\n",
    "the names containing the letter 'a', then output the number of names that\n",
    "contain 'a' and finally show me that new list.\"\"\"\n",
    "\n",
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=PROMPT,\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    "):\n",
    "    if chunk.candidates and chunk.candidates[0].content:\n",
    "        if chunk.candidates[0].content.parts is not None:\n",
    "            for part in chunk.candidates[0].content.parts:\n",
    "                if part.text:\n",
    "                    display(Markdown(\"#### Natural language stream\"))\n",
    "                    display(Markdown(part.text))\n",
    "                    display(Markdown(\"---\"))\n",
    "                if part.executable_code:\n",
    "                    display(Markdown(\"#### Code stream\"))\n",
    "                    display(\n",
    "                        Markdown(\n",
    "                            f\"\"\"\n",
    "    ```py\n",
    "    {part.executable_code.code}\n",
    "    ```\n",
    "    \"\"\"\n",
    "                        )\n",
    "                    )\n",
    "                    display(Markdown(\"---\"))\n",
    "                if part.code_execution_result:\n",
    "                    display(Markdown(\"#### Code result\"))\n",
    "                    display(\n",
    "                        Markdown(\n",
    "                            f\"\"\"\n",
    "    ```\n",
    "    {part.code_execution_result.output}\n",
    "    ```\n",
    "    \"\"\"\n",
    "                        )\n",
    "                    )\n",
    "                    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "This streaming example demonstrated how the Gemini API can generate, execute code, and provide results within a streaming session.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution) for more details about code execution, and in particular, the [recommendations](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution#code-execution-vs-function-calling) regarding differences between code execution and [function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling).\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/)\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai)\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YZNpgtKJDdPZ"
   ],
   "name": "intro_code_execution.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m139",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m139"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
